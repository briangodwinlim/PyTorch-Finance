{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "import onnxruntime as ort\n",
    "import plotly.express as px\n",
    "from hydra import compose, initialize\n",
    "\n",
    "os.chdir('..')\n",
    "from src.utils import train_setup, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify metrics and device\n",
    "metrics = {'mse': nn.MSELoss(), 'mae': nn.L1Loss()}\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790e39f",
   "metadata": {},
   "source": [
    "## Model Inference with MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842de0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path='../config'):\n",
    "    # Load config/inference.yaml\n",
    "    config = compose(config_name='inference.yaml')\n",
    "    \n",
    "    # Load model from MLFlow\n",
    "    mlflow.set_tracking_uri(uri=config.mlflow.uri)\n",
    "    model = mlflow.pytorch.load_model(config.mlflow.model_uri).to(device)\n",
    "    \n",
    "    # Set-up datasets and loaders\n",
    "    datasets, loaders, _ = train_setup(config, device)\n",
    "    train_dataset, val_dataset, test_dataset = datasets\n",
    "    train_loader, val_loader, test_loader = loaders\n",
    "\n",
    "# Evaluate model performance on test data\n",
    "test_metrics, test_outputs = evaluate(model, test_loader, device, metrics, train_dataset.inverse_transform_targets, config)\n",
    "test_df = pd.DataFrame({k: np.squeeze(v) for k, v in test_outputs.items()} | {'dt': test_dataset.index}).set_index('dt')\n",
    "pprint(test_metrics)\n",
    "\n",
    "# Plot model predictions\n",
    "fig = px.line(test_df.reset_index(), x='dt', y=['targets', 'predictions'], title='Model Prediction on Test Dataset', template='plotly_white')\n",
    "fig.update_traces(mode='lines')\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='', legend_title_text='')\n",
    "fig.update_traces(selector=dict(name='targets'), name='Actual', line=dict(color='black', dash='solid'))\n",
    "fig.update_traces(selector=dict(name='predictions'), name='Prediction', line=dict(color='blue', dash='solid'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea49952",
   "metadata": {},
   "source": [
    "## Model Inference with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a56e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path='../config'):\n",
    "    # Load config/inference.yaml\n",
    "    config = compose(config_name='inference.yaml')\n",
    "    \n",
    "    # Set-up datasets and loaders\n",
    "    datasets, loaders, _ = train_setup(config, device)\n",
    "    train_dataset, val_dataset, test_dataset = datasets\n",
    "    train_loader, val_loader, test_loader = loaders\n",
    "    \n",
    "# Load model from ONNX\n",
    "model_path = os.path.join(config.logging.model_dir, config.logging._exp_name, config.logging.model_file)\n",
    "ort_session = ort.InferenceSession(model_path)\n",
    "\n",
    "# Evaluate model performance on test data\n",
    "test_features = torch.stack([test_dataset[idx][0] for idx in range(len(test_dataset))], dim=0)\n",
    "test_targets = torch.stack([test_dataset[idx][1] for idx in range(len(test_dataset))], dim=0)\n",
    "test_predictions = torch.from_numpy(ort_session.run(None, {'features': test_features.cpu().numpy()})[0])\n",
    "test_targets = train_dataset.inverse_transform_targets(test_targets)\n",
    "test_predictions = train_dataset.inverse_transform_targets(test_predictions)\n",
    "test_metrics = {metric: metric_fn(test_targets, test_predictions).item() \n",
    "                for metric, metric_fn in metrics.items()}\n",
    "test_df = pd.DataFrame({'targets': test_targets.squeeze().cpu().numpy(), \n",
    "                        'predictions': test_predictions.squeeze().cpu().numpy(),\n",
    "                        'dt': test_dataset.index}).set_index('dt')\n",
    "pprint(test_metrics)\n",
    "\n",
    "# Plot model predictions\n",
    "fig = px.line(test_df.reset_index(), x='dt', y=['targets', 'predictions'], title='Model Prediction on Test Dataset', template='plotly_white')\n",
    "fig.update_traces(mode='lines')\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='', legend_title_text='')\n",
    "fig.update_traces(selector=dict(name='targets'), name='Actual', line=dict(color='black', dash='solid'))\n",
    "fig.update_traces(selector=dict(name='predictions'), name='Prediction', line=dict(color='blue', dash='solid'))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
